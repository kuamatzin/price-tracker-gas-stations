# Story 1.6: Scraper-to-Laravel Integration

## Status

Approved

## Story

**As a** system integrator,
**I want** the Node.js scraper to communicate with Laravel API after each run,
**so that** Laravel maintains awareness of data collection status.

## Acceptance Criteria

1. Scraper calls Laravel webhook endpoint upon completion with summary statistics
2. Laravel stores scraper run history including start time, end time, records processed, and changes detected
3. Failed scraper runs trigger Laravel error handling and admin notifications
4. Manual trigger endpoint in Laravel can initiate scraper run on-demand
5. Laravel can query current data freshness and alert if data is stale (>25 hours old)
6. Integration includes proper authentication between services using API keys
7. Webhook authentication uses HMAC-SHA256 signature verification with shared secret
8. Database connection pool configured with min 2, max 10 connections
9. Scraper health metrics exposed at /metrics endpoint for monitoring

## Tasks / Subtasks

- [ ] **Task 1: Create webhook endpoint in Laravel** (AC: 1, 2, 7)
  - [ ] Create ScraperController in app/Http/Controllers/Webhooks/
  - [ ] Implement POST /api/webhooks/scraper/complete endpoint
  - [ ] Add webhook route in routes/api/v1.php
  - [ ] Create ScraperRun model and repository
  - [ ] Implement HMAC-SHA256 signature verification middleware

- [ ] **Task 2: Implement webhook authentication** (AC: 6, 7)
  - [ ] Create VerifyWebhookSignature middleware
  - [ ] Generate shared secret and store in .env (SCRAPER_WEBHOOK_SECRET)
  - [ ] Implement HMAC-SHA256 signature generation/verification
  - [ ] Add timing attack protection using hash_equals()
  - [ ] Log authentication failures for security monitoring

- [ ] **Task 3: Create scraper run tracking** (AC: 2)
  - [ ] Implement ScraperRunService for business logic
  - [ ] Create methods to store run statistics in scraper_runs table
  - [ ] Track: started_at, completed_at, status, statistics
  - [ ] Calculate and store execution duration
  - [ ] Implement error tracking with detailed JSON logs

- [ ] **Task 4: Build manual trigger endpoint** (AC: 4)
  - [ ] Create POST /api/v1/scraper/trigger endpoint
  - [ ] Implement ScraperTriggerCommand using Process facade
  - [ ] Add authentication requirement (future: admin only)
  - [ ] Queue the scraper execution to avoid timeout
  - [ ] Return job ID for status tracking

- [ ] **Task 5: Implement data freshness monitoring** (AC: 5)
  - [ ] Create DataFreshnessService
  - [ ] Query last successful scraper run from database
  - [ ] Calculate hours since last successful run
  - [ ] Create alert if data older than 25 hours
  - [ ] Add freshness check to health endpoint

- [ ] **Task 6: Add webhook client to scraper** (AC: 1)
  - [ ] Install axios in Node.js scraper
  - [ ] Create webhook client module in scraper
  - [ ] Implement HMAC signature generation
  - [ ] Send completion webhook with statistics
  - [ ] Add retry logic for webhook failures

- [ ] **Task 7: Configure database connection pool** (AC: 8)
  - [ ] Update Node.js scraper database configuration
  - [ ] Set connection pool min: 2, max: 10
  - [ ] Configure idle timeout and connection timeout
  - [ ] Add connection pool monitoring
  - [ ] Test pool behavior under load

- [ ] **Task 8: Create metrics endpoint in scraper** (AC: 9)
  - [ ] Implement GET /metrics endpoint in Node.js scraper
  - [ ] Expose Prometheus-compatible metrics
  - [ ] Include: execution time, success rate, API calls made
  - [ ] Add current pool connections and memory usage
  - [ ] Document metrics format for monitoring tools

- [ ] **Task 9: Implement error handling and notifications** (AC: 3)
  - [ ] Create ScraperFailureJob for async processing
  - [ ] Implement admin notification via configured channel
  - [ ] Log detailed error information to Sentry
  - [ ] Create failure recovery mechanism
  - [ ] Add exponential backoff for retries

- [ ] **Task 10: Create API token management** (AC: 6)
  - [ ] Generate API token for scraper service
  - [ ] Store token in api_tokens table
  - [ ] Create middleware to validate service tokens
  - [ ] Implement token rotation mechanism
  - [ ] Add rate limiting per token

- [ ] **Task 11: Build integration monitoring** (AC: 2, 5, 9)
  - [ ] Add integration status to main health endpoint
  - [ ] Create dashboard view in Telescope for scraper runs
  - [ ] Implement alerting rules for failures
  - [ ] Add performance metrics tracking
  - [ ] Create integration test suite

## Dev Notes

### Previous Story Context

[Source: Stories 1.2, 1.4, 1.5]

- Story 1.2: Created scraper_runs and api_tokens tables
- Story 1.4: Implemented Node.js scraper with statistics tracking
- Story 1.5: Established Laravel foundation with health endpoint and scheduler

### Integration Architecture

[Source: architecture/core-workflows.md#price-scraping-workflow]

**Webhook Flow:**

1. Scraper completes execution
2. Calculates statistics (stations found, changes detected)
3. Generates HMAC signature using shared secret
4. POSTs to Laravel webhook with stats
5. Laravel verifies signature
6. Stores run information in scraper_runs table
7. Triggers any follow-up jobs (future: alerts evaluation)

### Webhook Specification

[Source: architecture/components.md]

**POST /api/webhooks/scraper/complete**

Request Headers:

```
X-Webhook-Signature: sha256=<hmac_signature>
Content-Type: application/json
```

Request Body:

```json
{
  "started_at": "2024-01-13T05:00:00Z",
  "completed_at": "2024-01-13T05:45:00Z",
  "status": "completed",
  "statistics": {
    "estados_processed": 32,
    "municipios_processed": 2456,
    "stations_found": 12543,
    "price_changes_detected": 1847,
    "new_stations_added": 5,
    "errors_encountered": 3
  },
  "errors": [
    {
      "type": "API_ERROR",
      "endpoint": "municipios",
      "message": "Timeout after 30s",
      "estado_id": 15
    }
  ]
}
```

Response:

```json
{
  "status": "success",
  "scraper_run_id": 123,
  "next_run_scheduled": "2024-01-14T05:00:00Z"
}
```

### HMAC Authentication

[Source: AC 7]

**Signature Generation (Node.js):**

```javascript
const crypto = require("crypto");
const signature = crypto
  .createHmac("sha256", process.env.WEBHOOK_SECRET)
  .update(JSON.stringify(payload))
  .digest("hex");
headers["X-Webhook-Signature"] = `sha256=${signature}`;
```

**Signature Verification (Laravel):**

```php
$payload = $request->getContent();
$signature = hash_hmac('sha256', $payload, config('scraper.webhook_secret'));
$expected = 'sha256=' . $signature;
$provided = $request->header('X-Webhook-Signature');

if (!hash_equals($expected, $provided)) {
    throw new UnauthorizedHttpException('Invalid signature');
}
```

### Manual Trigger Endpoint

[Source: AC 4]

**POST /api/v1/scraper/trigger**

Request:

```json
{
  "dry_run": false,
  "force": true
}
```

Response:

```json
{
  "status": "queued",
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "message": "Scraper execution queued"
}
```

Implementation uses Laravel Process:

```php
Process::path(base_path('../scraper'))
    ->start('npm run scrape');
```

### Data Freshness Monitoring

[Source: AC 5]

Query for freshness check:

```sql
SELECT
    completed_at,
    EXTRACT(EPOCH FROM (NOW() - completed_at))/3600 as hours_ago
FROM scraper_runs
WHERE status = 'completed'
ORDER BY completed_at DESC
LIMIT 1;
```

Alert threshold: 25 hours (allowing for 1-hour execution time)

### Database Connection Pool

[Source: AC 8, architecture/components.md]

Node.js scraper pool configuration:

```javascript
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  min: 2,
  max: 10,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});
```

### Metrics Endpoint

[Source: AC 9]

**GET /metrics** (Prometheus format):

```
# HELP scraper_runs_total Total number of scraper runs
# TYPE scraper_runs_total counter
scraper_runs_total{status="completed"} 145
scraper_runs_total{status="failed"} 3

# HELP scraper_duration_seconds Scraper execution duration
# TYPE scraper_duration_seconds histogram
scraper_duration_seconds_bucket{le="1800"} 142
scraper_duration_seconds_bucket{le="3600"} 148

# HELP scraper_price_changes_total Total price changes detected
# TYPE scraper_price_changes_total counter
scraper_price_changes_total 28453

# HELP database_pool_active Active database connections
# TYPE database_pool_active gauge
database_pool_active 3
```

### API Token Configuration

[Source: Story 1.2, AC 6]

Service token stored in api_tokens table:

- name: "scraper-service"
- abilities: ["webhook:complete", "metrics:read"]
- No expiration for service tokens
- Rate limit: 1000 requests/hour

### Environment Variables

**Laravel (apps/api/.env):**

```
SCRAPER_WEBHOOK_SECRET=<generate-strong-secret>
SCRAPER_COMMAND="cd ../scraper && npm run scrape"
SCRAPER_TIMEOUT=3600
DATA_STALE_HOURS=25
```

**Node.js Scraper (apps/scraper/.env):**

```
WEBHOOK_URL=http://localhost:8000/api/webhooks/scraper/complete
WEBHOOK_SECRET=<same-as-laravel>
API_TOKEN=<from-api_tokens-table>
DATABASE_POOL_MIN=2
DATABASE_POOL_MAX=10
METRICS_PORT=9090
```

### Error Handling Strategy

[Source: AC 3]

1. Scraper fails â†’ Sends webhook with status: "failed"
2. Laravel receives failure webhook
3. Queues ScraperFailureJob
4. Job sends notifications:
   - Log to Sentry with full context
   - Future: Email to admin
   - Future: Telegram message to admin
5. Implements retry with exponential backoff

## Testing

### Testing Standards

[Source: architecture/testing-strategy.md]

- Feature tests in apps/api/tests/Feature/
- Integration tests for webhook flow
- Mock external services where appropriate
- Test both success and failure scenarios

### Specific Test Cases

1. Test webhook endpoint accepts valid signature
2. Verify webhook rejects invalid signature
3. Test scraper run data stored correctly
4. Verify manual trigger queues job
5. Test data freshness calculation accuracy
6. Verify stale data alert triggers after 25 hours
7. Test database pool respects min/max connections
8. Verify metrics endpoint returns Prometheus format
9. Test error handling for failed scraper runs
10. Verify API token authentication works
11. Test HMAC timing attack protection
12. Verify integration with health endpoint

## Change Log

| Date       | Version | Description            | Author      |
| ---------- | ------- | ---------------------- | ----------- |
| 2025-08-13 | 1.0     | Initial story creation | BMad Master |

## Dev Agent Record

### Agent Model Used

(To be filled by dev agent)

### Debug Log References

(To be filled by dev agent)

### Completion Notes List

(To be filled by dev agent)

### File List

(To be filled by dev agent)

## QA Results

(To be filled by QA agent)
