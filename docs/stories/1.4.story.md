# Story 1.4: Government API Scraper Implementation

## Status

Approved

## Story

**As a** data analyst,
**I want** an automated Node.js scraper that collects pricing data from the government API,
**so that** our system always has current pricing information.

## Acceptance Criteria

1. Scraper fetches all 32 estados from api-catalogo.cne.gob.mx/api/utiles/entidadesfederativas then iterates through each estado to get municipios from api-catalogo.cne.gob.mx/api/utiles/municipios?EntidadFederativaId={id}
2. For each municipio, scraper calls api-reportediario.cne.gob.mx/api/EstacionServicio/Petroliferos?entidadId={estado}&municipioId={municipio} to retrieve pricing data for all stations
3. Parser correctly extracts station data (Numero as station_id, Nombre, Direccion, lat/lng if available) and price data (Producto type mapping Diésel/Gasolinas, SubProducto for specific fuel grade, PrecioVigente as current price)
4. Change detection compares PrecioVigente against last stored price for each station/fuel combination, only creating new price_changes records when values differ
5. Scraper handles API failures gracefully with exponential backoff retry logic (base delay 1s, multiplier 2, max delay 30s, max 5 retries) and continues processing remaining municipios if one fails
6. Detailed logging captures estados processed, municipios checked, total stations found, price changes detected, and any API errors with specific endpoint details
7. Rate limiting implemented for government API calls (max 10 requests/second with 100ms delay between calls)
8. Circuit breaker pattern implemented to halt scraping after 10 consecutive failures

## Tasks / Subtasks

- [x] **Task 1: Set up Node.js scraper project structure** (AC: 1)
  - [x] Initialize Node.js project in apps/scraper/
  - [x] Configure TypeScript with proper compilation settings
  - [x] Install required dependencies: got@13.0+, node-postgres, winston, p-limit
  - [x] Create folder structure: src/scrapers/, src/db/, src/utils/, src/config.ts
  - [x] Set up environment variables from .env file

- [ ] **Task 2: Implement database connection layer** (AC: 4)
  - [ ] Create PostgreSQL connection pool with min 2, max 10 connections
  - [ ] Implement database queries for fetching last prices by station
  - [ ] Create insert methods for new stations and price_changes
  - [ ] Add transaction support for batch inserts
  - [ ] Implement connection retry logic

- [ ] **Task 3: Build government API client** (AC: 1, 2, 5, 7)
  - [ ] Create HTTP client wrapper using Got with retry configuration
  - [ ] Implement rate limiter using p-limit (max 10 requests/second)
  - [ ] Add exponential backoff retry logic (1s base, 2x multiplier, 30s max, 5 retries)
  - [ ] Create methods for each API endpoint (estados, municipios, petroliferos)
  - [ ] Add request/response logging with timing

- [ ] **Task 4: Implement data fetching logic** (AC: 1, 2)
  - [ ] Create scraper for fetching all 32 estados
  - [ ] Implement municipio fetcher for each estado
  - [ ] Build station price fetcher for each municipio
  - [ ] Handle pagination if API returns paginated results
  - [ ] Add progress tracking for monitoring

- [ ] **Task 5: Create data parser and mapper** (AC: 3)
  - [ ] Parse station data: Numero, Nombre, Direccion, coordinates
  - [ ] Extract fuel prices from Producto/SubProducto/PrecioVigente
  - [ ] Implement fuel type mapping logic (Regular/Premium/Diesel)
  - [ ] Handle missing or malformed data gracefully
  - [ ] Validate data types and ranges

- [ ] **Task 6: Implement change detection algorithm** (AC: 4)
  - [ ] Query existing prices for each station/fuel combination
  - [ ] Compare new prices with last stored prices
  - [ ] Create price_changes records only when prices differ
  - [ ] Track new stations (not in database)
  - [ ] Handle station updates (name, address changes)

- [ ] **Task 7: Add circuit breaker pattern** (AC: 8)
  - [ ] Track consecutive API failures
  - [ ] Implement circuit breaker with 10 failure threshold
  - [ ] Add circuit breaker states: closed, open, half-open
  - [ ] Create recovery mechanism after cooldown period
  - [ ] Log circuit breaker state changes

- [ ] **Task 8: Implement comprehensive logging** (AC: 6)
  - [ ] Configure Winston with daily rotation and error files
  - [ ] Log scraper start/end times
  - [ ] Track estados processed, municipios checked
  - [ ] Count stations found and price changes detected
  - [ ] Log all API errors with endpoint and error details
  - [ ] Create summary statistics at completion

- [ ] **Task 9: Create scraper orchestrator** (AC: 1, 2, 4, 5)
  - [ ] Build main orchestration logic to coordinate all scrapers
  - [ ] Implement parallel processing with concurrency control
  - [ ] Add error recovery to continue after failures
  - [ ] Create progress reporting during execution
  - [ ] Handle graceful shutdown on SIGTERM/SIGINT

- [ ] **Task 10: Add monitoring and health endpoints** (AC: 6, 8)
  - [ ] Create /health endpoint for monitoring
  - [ ] Expose /metrics endpoint with scraper statistics
  - [ ] Track execution time, success rate, error count
  - [ ] Monitor memory usage and database connections
  - [ ] Add alerting hooks for critical failures

- [ ] **Task 11: Create manual trigger and scheduling** (AC: 1-8)
  - [ ] Add script for manual execution: npm run scrape
  - [ ] Create Docker entrypoint for container execution
  - [ ] Document environment variables required
  - [ ] Add dry-run mode for testing without database writes
  - [ ] Create integration with Laravel scheduler (future story)

## Dev Notes

### Previous Story Context

[Source: Story 1.1, 1.2, 1.3]

- Story 1.1: Monorepo structure created with apps/scraper/ directory
- Story 1.2: Database schema created with all tables including price_changes and scraper_runs
- Story 1.3: External services configured including Sentry for error tracking

### Scraper Architecture

[Source: architecture/components.md#scraper-service-nodejs]

**Technology Stack:**

- Node.js 20 LTS (runtime)
- TypeScript (language)
- Got 13.0+ (HTTP client with retry support)
- node-postgres (PostgreSQL client)
- Winston (logging with rotation)
- p-limit (rate limiting)

**Key Interfaces:**

- HTTP client for government API calls
- PostgreSQL connection for direct price writes
- Webhook POST to Laravel API upon completion (future story)
- Health check endpoint at /health
- Metrics endpoint at /metrics

### Government API Specification

[Source: architecture/external-apis.md#government-pricing-api]

**API Endpoints:**

1. **Catalog API Base:** `https://api-catalogo.cne.gob.mx/api/utiles`
   - GET `/entidadesfederativas` - Returns 32 states
   - GET `/municipios?EntidadFederativaId={id}` - Returns municipalities

2. **Pricing API Base:** `https://api-reportediario.cne.gob.mx/api/EstacionServicio`
   - GET `/Petroliferos?entidadId={estado}&municipioId={municipio}`
   - Returns station data with pricing

**Response Fields:**

- `Numero`: Station permit number (unique ID)
- `Nombre`: Station name
- `Direccion`: Physical address
- `Producto`: Product type (Diésel/Gasolinas)
- `SubProducto`: Detailed fuel specification
- `PrecioVigente`: Current price

**Total API Calls:** ~2,500+ per complete run (32 states + ~2,400 municipalities)

### Fuel Type Mapping

[Source: architecture/external-apis.md#fuel-type-mapping-logic]

Map SubProducto variations to normalized types:

- **Regular:** "Regular (con un índice de octano ([RON+MON]/2) mínimo de 87)" → 'regular'
- **Premium:** "Premium (con un índice de octano ([RON+MON]/2) mínimo de 91)" → 'premium'
- **Diesel:** "Diésel" or variants → 'diesel'

Store normalized value in fuel_type, preserve original in subproducto field.

### Database Integration

[Source: Story 1.2, architecture/database-schema.md]

**Tables to interact with:**

1. **stations**: Upsert station data (numero, nombre, direccion)
2. **price_changes**: Insert only when price changes
3. **scraper_runs**: Update with execution statistics (future integration)

**Change Detection Query:**

```sql
SELECT station_numero, fuel_type, price, changed_at
FROM price_changes pc1
WHERE (station_numero, fuel_type, changed_at) IN (
    SELECT station_numero, fuel_type, MAX(changed_at)
    FROM price_changes
    GROUP BY station_numero, fuel_type
)
```

### Price Scraping Workflow

[Source: architecture/core-workflows.md#price-scraping-workflow]

1. Scraper triggered (manual or scheduled)
2. Fetch all 32 estados
3. For each estado, fetch municipios
4. For each municipio, fetch station prices
5. Query last prices from database
6. Compare and detect changes
7. Insert only changed prices
8. Update scraper_runs table (future)
9. POST webhook to Laravel (future)

### Error Handling Strategy

[Source: AC 5, 7, 8]

**Exponential Backoff:**

- Base delay: 1 second
- Multiplier: 2
- Max delay: 30 seconds
- Max retries: 5

**Rate Limiting:**

- Max 10 requests/second
- 100ms minimum delay between calls

**Circuit Breaker:**

- Opens after 10 consecutive failures
- Cooldown period: 5 minutes
- Half-open state for testing recovery

### Logging Requirements

[Source: AC 6, architecture/components.md]

**Log Levels:**

- ERROR: API failures, database errors
- WARN: Retries, circuit breaker trips
- INFO: Progress updates, statistics
- DEBUG: Individual API calls, data parsing

**Statistics to Track:**

- Estados processed: X/32
- Municipios processed: X/~2400
- Stations found: X
- Price changes detected: X
- New stations added: X
- Errors encountered: X

### Environment Variables

[Source: Story 1.1, 1.3]

Required in apps/scraper/.env:

```
DATABASE_URL=postgresql://user:pass@localhost:5432/fuelintel
NODE_ENV=development
LOG_LEVEL=info
MAX_RETRIES=5
RATE_LIMIT=10
SENTRY_DSN=xxx (from Story 1.3)
DRY_RUN=false
```

### Project Structure

[Source: architecture/unified-project-structure.md#lines-52-65]

```
apps/scraper/
├── src/
│   ├── scrapers/
│   │   ├── estados.ts
│   │   ├── municipios.ts
│   │   └── prices.ts
│   ├── db/
│   │   ├── connection.ts
│   │   └── queries.ts
│   ├── utils/
│   │   ├── logger.ts
│   │   ├── rateLimiter.ts
│   │   └── circuitBreaker.ts
│   ├── config.ts
│   └── index.ts
├── tests/
├── .env.example
├── tsconfig.json
└── package.json
```

## Testing

### Testing Standards

[Source: architecture/testing-strategy.md]

- Unit tests in apps/scraper/tests/unit/
- Integration tests in apps/scraper/tests/integration/
- Use Jest for test runner
- Mock external API calls in tests
- Test database operations with test database

### Specific Test Cases

1. Test fuel type mapping for all known SubProducto variants
2. Verify exponential backoff increases delays correctly
3. Test circuit breaker opens after 10 failures
4. Verify change detection only creates records for actual changes
5. Test rate limiter enforces 10 requests/second limit
6. Verify scraper continues after individual municipio failures
7. Test database connection pool management
8. Verify logging captures all required statistics
9. Test graceful shutdown on SIGTERM
10. Verify dry-run mode doesn't write to database

## Change Log

| Date       | Version | Description            | Author      |
| ---------- | ------- | ---------------------- | ----------- |
| 2025-08-13 | 1.0     | Initial story creation | BMad Master |

## Dev Agent Record

### Agent Model Used

claude-opus-4-1-20250805

### Debug Log References

(To be filled by dev agent)

### Completion Notes List

(To be filled by dev agent)

### File List

(To be filled by dev agent)

## QA Results

(To be filled by QA agent)
